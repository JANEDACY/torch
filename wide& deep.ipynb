{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e12756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4.81k/4.81k [00:01<00:00, 3.30kKB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userID  itemID  rating                             genre\n",
      "0     196     242     3.0                            Comedy\n",
      "1     186     302     3.0  Crime|Film-Noir|Mystery|Thriller\n",
      "2      22     377     1.0                 Children's|Comedy\n",
      "3     244      51     2.0         Drama|Romance|War|Western\n",
      "4     166     346     1.0                       Crime|Drama\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from recommenders.datasets import movielens\n",
    "\n",
    "# 加载MovieLens数据\n",
    "data = movielens.load_pandas_df(\n",
    "    size=\"100k\",\n",
    "    header=[\"userID\", \"itemID\", \"rating\"],\n",
    "    genres_col=\"genre\"\n",
    ")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1574bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电影类型: ['Action' 'Adventure' 'Animation' \"Children's\" 'Comedy' 'Crime'\n",
      " 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'Musical' 'Mystery'\n",
      " 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western' 'unknown']\n",
      "   userID  itemID  rating                                              genre\n",
      "0     196     242     3.0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1     186     302     3.0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...\n",
      "2      22     377     1.0  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3     244      51     2.0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...\n",
      "4     166     346     1.0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "# 将电影类型编码为多热向量\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "data[\"genre\"] = genres_encoder.fit_transform(\n",
    "    data[\"genre\"].apply(lambda s: s.split(\"|\"))\n",
    ").tolist()\n",
    "\n",
    "print(\"电影类型:\", genres_encoder.classes_)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2161fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000 条训练样本和 25000 条测试样本\n"
     ]
    }
   ],
   "source": [
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "\n",
    "train, test = python_random_split(data, ratio=0.75, seed=42)\n",
    "print(f\"{len(train)} 条训练样本和 {len(test)} 条测试样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2f653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wide_features():\n",
    "    crossed_feature = tf.feature_column.crossed_column(\n",
    "        [\"userID\", \"itemID\"], hash_bucket_size=1000)\n",
    "    return [crossed_feature]\n",
    "def build_deep_features(user_dim, item_dim):\n",
    "    user_col = tf.feature_column.categorical_column_with_identity(\"userID\", num_buckets=1000)\n",
    "    item_col = tf.feature_column.categorical_column_with_identity(\"itemID\", num_buckets=2000)\n",
    "    \n",
    "    user_emb = tf.feature_column.embedding_column(user_col, dimension=user_dim)\n",
    "    item_emb = tf.feature_column.embedding_column(item_col, dimension=item_dim)\n",
    "    \n",
    "    genre_indicator = tf.feature_column.numeric_column(\"genre\", shape=(19,))\n",
    "    \n",
    "    return [user_emb, item_emb, genre_indicator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc51c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdisable_v2_behavior()\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241m.\u001b[39mDNNLinearCombinedRegressor(\n\u001b[0;32m      4\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39mmodel_dir,\n\u001b[0;32m      5\u001b[0m     linear_feature_columns\u001b[38;5;241m=\u001b[39mbuild_wide_features(),\n\u001b[0;32m      6\u001b[0m     dnn_feature_columns\u001b[38;5;241m=\u001b[39mbuild_deep_features(DNN_USER_DIM, DNN_ITEM_DIM),\n\u001b[0;32m      7\u001b[0m     dnn_hidden_units\u001b[38;5;241m=\u001b[39mDNN_HIDDEN_UNITS,\n\u001b[0;32m      8\u001b[0m     dnn_optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mAdagradOptimizer(learning_rate\u001b[38;5;241m=\u001b[39mDNN_OPTIMIZER_LR),\n\u001b[0;32m      9\u001b[0m     linear_optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFtrlOptimizer(learning_rate\u001b[38;5;241m=\u001b[39mLINEAR_OPTIMIZER_LR),\n\u001b[0;32m     10\u001b[0m     dnn_dropout\u001b[38;5;241m=\u001b[39mDNN_DROPOUT,\n\u001b[0;32m     11\u001b[0m     batch_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(DNN_BATCH_NORM),\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'estimator'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "    model_dir=model_dir,\n",
    "    \n",
    "    linear_feature_columns=build_wide_features(),\n",
    "    \n",
    "    dnn_feature_columns=build_deep_features(DNN_USER_DIM, DNN_ITEM_DIM),\n",
    "    \n",
    "    dnn_hidden_units=DNN_HIDDEN_UNITS,\n",
    "    \n",
    "    dnn_optimizer=tf.compat.v1.train.AdagradOptimizer(learning_rate=DNN_OPTIMIZER_LR),\n",
    "    linear_optimizer=tf.compat.v1.train.FtrlOptimizer(learning_rate=LINEAR_OPTIMIZER_LR),\n",
    "    dnn_dropout=DNN_DROPOUT,\n",
    "    batch_norm=bool(DNN_BATCH_NORM),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3c61230",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mpandas_input_fn(\n\u001b[0;32m      4\u001b[0m         x\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m      5\u001b[0m         y\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(input_fn\u001b[38;5;241m=\u001b[39minput_fn(train, BATCH_SIZE), steps\u001b[38;5;241m=\u001b[39mSTEPS)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m     15\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(input_fn\u001b[38;5;241m=\u001b[39minput_fn(test, BATCH_SIZE), steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 定义输入函数\n",
    "def input_fn(data, batch_size):\n",
    "    return tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "        x=data,\n",
    "        y=data[\"rating\"],\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=None,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "# 训练模型\n",
    "model.train(input_fn=input_fn(train, BATCH_SIZE), steps=STEPS)\n",
    "\n",
    "# 评估模型\n",
    "results = model.evaluate(input_fn=input_fn(test, BATCH_SIZE), steps=None)\n",
    "print(\"测试集上的RMSE:\", results[\"average_loss\"]**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61877185",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3088309089.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    ......\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class EvalRecordHook(tf.estimator.SessionRunHook):\n",
    "    def __init__(self, log_path, every_n_iter):\n",
    "        self.log_path = log_path\n",
    "        self.every_n_iter = every_n_iter\n",
    "        self.records = []\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        if run_context.session.run(tf.compat.v1.train.get_global_step()) % self.every_n_iter == 0:\n",
    "            eval_results = model.evaluate(input_fn=input_fn(test, BATCH_SIZE), steps=None)\n",
    "            self.records.append({\n",
    "                \"step\": run_context.session.run(tf.compat.v1.train.get_global_step()),\n",
    "                \"rmse\": eval_results[\"average_loss\"]**0.5\n",
    "            })\n",
    "            with open(self.log_path, \"w\") as f:\n",
    "                json.dump(self.records, f)\n",
    "\n",
    "eval_hook = EvalRecordHook(os.path.join(model_dir, \"eval_records.json\"), 1000)\n",
    "model.train(input_fn=input_fn(train, BATCH_SIZE), steps=STEPS, hooks=[eval_hook])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "with open(os.path.join(model_dir, \"eval_records.json\"), \"r\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "steps = [record[\"step\"] for record in records]\n",
    "rmse = [record[\"rmse\"] for record in records]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, rmse)\n",
    "plt.title(\"模型训练过程中RMSE的变化\")\n",
    "plt.xlabel(\"训练步数\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6cb5cb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrecommenders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_evaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     rmse, mae, ndcg_at_k, precision_at_k\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 获取测试集上的预测结果\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(input_fn\u001b[38;5;241m=\u001b[39minput_fn(test, BATCH_SIZE))\n\u001b[0;32m      7\u001b[0m pred_ratings \u001b[38;5;241m=\u001b[39m [p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 计算各种评估指标\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from recommenders.evaluation.python_evaluation import (\n",
    "    rmse, mae, ndcg_at_k, precision_at_k\n",
    ")\n",
    "\n",
    "# 获取测试集上的预测结果\n",
    "predictions = model.predict(input_fn=input_fn(test, BATCH_SIZE))\n",
    "pred_ratings = [p['predictions'][0] for p in predictions]\n",
    "\n",
    "# 计算各种评估指标\n",
    "print(\"RMSE:\", rmse(test[\"rating\"], pred_ratings))\n",
    "print(\"MAE:\", mae(test[\"rating\"], pred_ratings))\n",
    "print(\"NDCG@10:\", ndcg_at_k(test, pred_ratings, k=10))\n",
    "print(\"Precision@10:\", precision_at_k(test, pred_ratings, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9aa323b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_movies\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 为用户196推荐10部电影\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m196\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m为用户196推荐的电影:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m movie, rating \u001b[38;5;129;01min\u001b[39;00m recommendations:\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[1;34m(user_id, top_k)\u001b[0m\n\u001b[0;32m      3\u001b[0m all_movies \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitemID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 构造用户-电影对\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m user_movie_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muserID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_movies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitemID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_movies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitemID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_movies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 使用模型预测评分\u001b[39;00m\n\u001b[0;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(input_fn\u001b[38;5;241m=\u001b[39minput_fn(user_movie_pairs, BATCH_SIZE))\n",
      "File \u001b[1;32mD:\\ana\\envs\\torch\\lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mD:\\ana\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ana\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mD:\\ana\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "def recommend_movies(user_id, top_k=10):\n",
    "    # 获取所有电影ID\n",
    "    all_movies = data[\"itemID\"].unique()\n",
    "    \n",
    "    # 构造用户-电影对\n",
    "    user_movie_pairs = pd.DataFrame({\n",
    "        \"userID\": [user_id] * len(all_movies),\n",
    "        \"itemID\": all_movies,\n",
    "        \"genre\": data[data[\"itemID\"].isin(all_movies)][\"genre\"].tolist()\n",
    "    })\n",
    "    \n",
    "    # 使用模型预测评分\n",
    "    predictions = model.predict(input_fn=input_fn(user_movie_pairs, BATCH_SIZE))\n",
    "    pred_ratings = [p['predictions'][0] for p in predictions]\n",
    "    \n",
    "    # 将预测结果与电影ID配对并排序\n",
    "    movie_ratings = list(zip(all_movies, pred_ratings))\n",
    "    top_movies = sorted(movie_ratings, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    return top_movies\n",
    "\n",
    "# 为用户196推荐10部电影\n",
    "recommendations = recommend_movies(196, top_k=10)\n",
    "print(\"为用户196推荐的电影:\")\n",
    "for movie, rating in recommendations:\n",
    "    print(f\"电影ID: {movie}, 预测评分: {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee4820b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recommendations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m电影ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 预测评分: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrating\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 新推荐\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m analyze_recommendations(\u001b[38;5;241m196\u001b[39m, \u001b[43mrecommendations\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recommendations' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_recommendations(user_id, recommendations):\n",
    "    user_history = data[data[\"userID\"] == user_id]\n",
    "    print(f\"用户{user_id}的历史评分:\")\n",
    "    print(user_history[[\"itemID\", \"rating\"]].to_string(index=False))\n",
    "    \n",
    "    print(\"\\n推荐电影:\")\n",
    "    for movie, rating in recommendations:\n",
    "        if movie in user_history[\"itemID\"].values:\n",
    "            actual_rating = user_history[user_history[\"itemID\"] == movie][\"rating\"].values[0]\n",
    "            print(f\"电影ID: {movie}, 预测评分: {rating:.2f}, 实际评分: {actual_rating}\")\n",
    "        else:\n",
    "            print(f\"电影ID: {movie}, 预测评分: {rating:.2f}, 新推荐\")\n",
    "\n",
    "analyze_recommendations(196, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a257b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e68aa9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v1' has no attribute 'estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\n",
      "File \u001b[1;32mD:\\ana\\envs\\torch\\lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py:232\u001b[0m, in \u001b[0;36mTFModuleWrapper._getattr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Imports and caches pre-defined API.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mWarns if necessary.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m   attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfmw_wrapped_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Placeholder for Google-internal contrib error\u001b[39;00m\n\u001b[0;32m    236\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfmw_public_apis:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v1' has no attribute 'estimator'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac0220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
